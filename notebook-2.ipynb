{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import re\n",
    "#import openpyxl\n",
    "#import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the station channels\n",
    "list_of_columns = pd.read_csv(\"data/nb_air_quality_station_channels.csv\")\n",
    "\n",
    "list_of_columns.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List of columns\n",
    "\n",
    "def create_list_of_columns(station_id):\n",
    "    cols_dynamic =[]\n",
    "    cols = [\"STATION_ID\",\"DATE_TIME\"]\n",
    "    \n",
    "    total_analytes = len(list_of_columns[list_of_columns[\"STATION_ID\"] == station_id])\n",
    "    #print(total_analytes)\n",
    "    for i in range(1,total_analytes+1,1):\n",
    "        analytes_name = list_of_columns[(list_of_columns[\"STATION_ID\"]==station_id) & (list_of_columns[\"CHANNEL\"] == i)].NAME.values[0] if len(list_of_columns[(list_of_columns[\"STATION_ID\"]==station_id) & (list_of_columns[\"CHANNEL\"] == i)]) > 0 else \"Error_loading_\"+str(station_id)\n",
    "        cols_dynamic.append(analytes_name)\n",
    "        cols_dynamic.append(str(analytes_name)+\"_FLAG\")\n",
    "        #print(\"%s | %s | %s\" % (analytes_name,station_id,i))\n",
    "    \n",
    "    cols.extend(cols_dynamic)\n",
    "    return cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_columns(current_dataframe):\n",
    "    indexes = []\n",
    "    for i in range(0,len(current_dataframe.columns),1):\n",
    "        if(len(current_dataframe[current_dataframe.columns[i]])==current_dataframe[current_dataframe.columns[i]].isna().sum()):\n",
    "            indexes.append(i)\n",
    "            \n",
    "    current_dataframe.drop(indexes,inplace=True, axis=1)\n",
    "    return current_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSV files from a folder\n",
    "\n",
    "current_directory = str(os.getcwd()) + \"\\\\raw_data\\\\\"\n",
    "dataframes = []\n",
    "\n",
    "all_files = []\n",
    "\n",
    "for path, subdirs, files in os.walk(current_directory):\n",
    "    for name in files:\n",
    "        file_name = os.path.join(path, name)\n",
    "        format_matches = [\".lsi\", \".csv\"]\n",
    "        exclue_matches = []\n",
    "        if name not in all_files:\n",
    "            if any([x in name for x in format_matches]):\n",
    "                if not any([y in file_name for y in exclue_matches]):\n",
    "                    try:\n",
    "                        current_dataframe = pd.read_csv(file_name, low_memory=False, header=None,sep=\",\")\n",
    "                        \n",
    "                        current_dataframe = current_dataframe.groupby([0])\n",
    "                        keys = current_dataframe.groups.keys()\n",
    "                        \n",
    "                        for index, x in enumerate(keys):\n",
    "                            temp_dataframe = current_dataframe.get_group(x).copy()\n",
    "                            temp_dataframe = drop_empty_columns(temp_dataframe)\n",
    "            \n",
    "                            cols = create_list_of_columns(x)\n",
    "                            temp_dataframe.columns = cols\n",
    "                            dataframes.append(temp_dataframe)\n",
    "                        pass\n",
    "                    except Exception as e:\n",
    "                        print(\"Error reading file: \" + file_name)\n",
    "                        print(e)\n",
    "                else:\n",
    "                    print(\"Files Excluded : \" + file_name)\n",
    "            else:\n",
    "                print(\"Non Excel File: \" + file_name)\n",
    "        \n",
    "        all_files.append(name)\n",
    "all_files = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[3].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[4].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[4].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if dataframes have the same columns\n",
    "\n",
    "if all([set(dataframes[0].columns) == set(df.columns) for df in dataframes]):\n",
    "    print('Datasets have the same columns')\n",
    "else:\n",
    "    print('Datasets do not have the same columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the columns names that have found in some columns but not in others. This way we can create those columns for all the dataframes\n",
    "\n",
    "columns = []\n",
    "    \n",
    "for x in range(0, len(dataframes), 1):\n",
    "    for y in range(0, len(dataframes), 1):\n",
    "        for z in range(0, len(dataframes[x].columns), 1):\n",
    "            #print(str(z) + \"||\"+ str(len(dataframes[y].columns))+ \"||\" + str(y))\n",
    "            if(dataframes[x].columns[z] in dataframes[y].columns):\n",
    "                pass\n",
    "            else:\n",
    "                if (dataframes[x].columns[z] in columns):\n",
    "                    pass\n",
    "                else:\n",
    "                    columns.append(dataframes[x].columns[z])\n",
    "                \n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the dataframes into one\n",
    "\n",
    "dataset_nb_air_quality_raw = pd.concat(dataframes)\n",
    "dataset_nb_air_quality_raw.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Combined Dataset to a CSV\n",
    "\n",
    "dataset_nb_air_quality_raw.to_csv(\"data/nb_air_quality_raw.csv\", sep=',',index=False,encoding='utf-8-sig')\n",
    "\n",
    "#Shape of row data\n",
    "dataset_nb_air_quality_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of the dataset\n",
    "\n",
    "nb_air_quality_p1 = dataset_nb_air_quality_raw.copy()\n",
    "nb_air_quality_p1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -9999 with NaN\n",
    "nb_air_quality_p1 = nb_air_quality_p1.replace(-9999,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datetime_format(dt_str):\n",
    "    formats_to_check = [\n",
    "        '%Y/%m/%d %I:%M:%S %p',\n",
    "        '%Y-%m-%d %I:%M:%S %p',\n",
    "        '%Y/%m/%d %H:%M:%S',\n",
    "        '%Y-%m-%d %H:%M:%S',\n",
    "        '%d/%m/%Y %I:%M:%S %p',\n",
    "        '%d-%m-%Y %I:%M:%S %p',\n",
    "        '%d/%m/%Y %H:%M:%S',\n",
    "        '%d-%m-%Y %H:%M:%S',\n",
    "        '%Y/%m/%d',\n",
    "        '%Y-%m-%d',\n",
    "        '%d/%m/%Y',\n",
    "        '%d-%m-%Y',\n",
    "    ]\n",
    "\n",
    "    for fmt in formats_to_check:\n",
    "        try:\n",
    "            datetime.datetime.strptime(dt_str, fmt)\n",
    "            return fmt\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "dt_str = \"2019/01/01 5:00:00 AM\"\n",
    "format_found = find_datetime_format(dt_str)\n",
    "if format_found:\n",
    "    print(f\"Format found: {format_found}\")\n",
    "else:\n",
    "    print(\"Format not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_air_quality_p1[\"DATE_TIME\"] = nb_air_quality[\"DATE_TIME\"].replace('24:00','00:00' , regex=True)\n",
    "\n",
    "nb_air_quality_p1[\"DATE_TIME\"] = pd.to_datetime(nb_air_quality_p1[\"DATE_TIME\"], format='%Y/%m/%d %I:%M:%S %p')\n",
    "\n",
    "nb_air_quality_p1[\"DATE\"] = nb_air_quality_p1[\"DATE_TIME\"].dt.date\n",
    "nb_air_quality_p1[\"YEAR\"] = nb_air_quality_p1[\"DATE_TIME\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p1.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Station information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "\n",
    "station_information = pd.read_csv(\"data/nb_air_quality_station_info.csv\")\n",
    "\n",
    "nb_air_quality_p1 = pd.merge(nb_air_quality_p1, station_information[[\"STATION_ID\",\"STATION_NAME\",\"LATITUDE\",\"LONGITUDE\",\"STATION_NAPS_ID\",\"ORGANIZATION\",\"CITY\"]],  how='left', left_on=['STATION_ID'], right_on = ['STATION_ID'])\n",
    "\n",
    "nb_air_quality_p1.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Units Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unit_column(unitName,station_id):\n",
    "    channel_series = list_of_columns[(list_of_columns[\"STATION_ID\"]==station_id) & (list_of_columns[\"NAME\"] == unitName)]\n",
    "    station_id_channel_series = channel_series[\"STATION_ID\"]\n",
    "    station_id_channel = station_id_channel_series.values[0] if len(station_id_channel_series) > 0 else np.nan\n",
    "    \n",
    "    if(math.isnan(station_id_channel) == False and station_id_channel==station_id):\n",
    "        analyte_unit = channel_series[\"UNITS_EUG\"]\n",
    "        analyte_avg = channel_series[\"AVERAGE_TYPE\"]\n",
    "        analyte_num_format = channel_series[\"NUMERIC_FORMAT\"]\n",
    "        analyte_low = channel_series[\"LOW_RANGE\"]\n",
    "        analyte_high = channel_series[\"HIGH_RANGE\"]\n",
    "        analyte_state = channel_series[\"STATE\"]\n",
    "        \n",
    "        return \"Unit: \" + str(analyte_unit.values[0]) + \", Average Type: \" + str(analyte_avg.values[0]) + \", Number Format: \" + str(analyte_num_format.values[0]) + \", Low Range: \" + str(analyte_low.values[0]) + \", High Range: \" + str(analyte_high.values[0]) + \", Station State: \" + str(analyte_state.values[0])  if len(analyte_unit) > 0 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_with_info = []\n",
    "\n",
    "nb_air_quality_group = nb_air_quality_p1.groupby([\"STATION_ID\"])\n",
    "\n",
    "keys = nb_air_quality_group.groups.keys()\n",
    "\n",
    "for index, x in enumerate(keys):\n",
    "    temp_dataframe = nb_air_quality_group.get_group(x).copy()\n",
    "    \n",
    "    temp_dataframe[\"SO2_INFO\"] = add_unit_column(\"SO2\",x)\n",
    "    temp_dataframe[\"O3_INFO\"] = add_unit_column(\"O3\",x)\n",
    "    temp_dataframe[\"CO_INFO\"] = add_unit_column(\"CO\",x)\n",
    "    temp_dataframe[\"TRS_INFO\"] = add_unit_column(\"TRS\",x)\n",
    "    temp_dataframe[\"NO2_INFO\"] = add_unit_column(\"NO2\",x)\n",
    "    temp_dataframe[\"NO_INFO\"] = add_unit_column(\"NO\",x)\n",
    "    temp_dataframe[\"NOX_INFO\"] = add_unit_column(\"NOX\",x)\n",
    "    #temp_dataframe[\"H2S_INFO\"] = add_unit_column(\"H2S\",x)\n",
    "    #temp_dataframe[\"AQHI_INFO\"] = add_unit_column(\"AQHI\",x)\n",
    "    #temp_dataframe[\"AQI_INFO\"] = add_unit_column(\"AQI\",x)\n",
    "    temp_dataframe[\"PM_25_BAM_INFO\"] = add_unit_column(\"PM25 BAM\",x)\n",
    "    temp_dataframe[\"PM_25_API_INFO\"] = add_unit_column(\"PM_2.5API\",x)\n",
    "    #temp_dataframe[\"PM_10_API_INFO\"] = add_unit_column(\"PM_10API\",x)\n",
    "    #temp_dataframe[\"PM_25_TEOM_INFO\"] = add_unit_column(\"PM25 TEOM\",x)\n",
    "    temp_dataframe[\"WIND_DIR_INFO\"] = add_unit_column(\"Wind Dir\",x)\n",
    "    temp_dataframe[\"WIND_SPEED_INFO\"] = add_unit_column(\"Wind Speed\",x)\n",
    "    dataframes_with_info.append(temp_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_with_info[1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_air_quality_p1 = pd.concat(dataframes_with_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p1['STATION_ID'] = nb_air_quality_p1['STATION_ID'].astype(str)\n",
    "#m = nb_air_quality_p1.STATION_ID.str.len().max()\n",
    "nb_air_quality_p1['STATION_ID'] = nb_air_quality_p1['STATION_ID'].str.rjust(3,\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "pd.set_option('display.max_rows',None)\n",
    "nb_air_quality_p1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create another copy of the dataset for futher pre-processing\n",
    "\n",
    "Some methods are slow when processing data. Creating a copy of a dataset will allow us not to run the entire code during data development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p2 = nb_air_quality_p1.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and remove null columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_columns_dataset(dataset):\n",
    "    indexes = []\n",
    "    for i in range(0,len(dataset.columns),1):\n",
    "        if(len(dataset[dataset.columns[i]])==dataset[dataset.columns[i]].isna().sum()):\n",
    "            indexes.append(dataset.columns[i])\n",
    "            print(dataset.columns[i])\n",
    "   \n",
    "    dataset.drop(indexes,inplace=True, axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p2 = drop_empty_columns_dataset(nb_air_quality_p2) #finish this later when we have whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"SO2\", \"O3\", \"CO\", \"TRS\", \"NO2\", \"NO\", \"NOX\", \"PM25 BAM\", \"PM_2.5API\", \"Wind Dir\", \"Wind Speed\"]\n",
    "\n",
    "nb_air_quality_p2[cols] = nb_air_quality_p2[cols].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove unit information field value where there is no analyte value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import flag data \n",
    "\n",
    "status_flags = pd.read_csv(\"data/nb_air_quality_status_code.csv\")\n",
    "\n",
    "get_flag = status_flags[(status_flags[\"DATA_STATUS_VALID\"]==1)]\n",
    "match_status_list = get_flag[\"STATUS_CODE\"].tolist()\n",
    "\n",
    "print(match_status_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unit_when_flag(unitVal, flagVal):\n",
    "    \n",
    "    #print(unitVal)\n",
    "    if flagVal in match_status_list:\n",
    "        return unitVal\n",
    "\n",
    "    return \"(\"+str(unitVal)+\")\" if pd.isnull(unitVal)==False else np.nan\n",
    "\n",
    "remove_unit_when_flag_vec = np.vectorize(remove_unit_when_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p2[\"SO2\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"SO2\"],nb_air_quality_p2[\"SO2_FLAG\"])\n",
    "nb_air_quality_p2[\"O3\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"O3\"],nb_air_quality_p2[\"O3_FLAG\"])\n",
    "nb_air_quality_p2[\"CO\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"CO\"],nb_air_quality_p2[\"CO_FLAG\"])\n",
    "nb_air_quality_p2[\"TRS\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"TRS\"],nb_air_quality_p2[\"TRS_FLAG\"])\n",
    "nb_air_quality_p2[\"NO2\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"NO2\"],nb_air_quality_p2[\"NO2_FLAG\"])\n",
    "nb_air_quality_p2[\"NO\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"NO\"],nb_air_quality_p2[\"NO_FLAG\"])\n",
    "nb_air_quality_p2[\"NOX\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"NOX\"],nb_air_quality_p2[\"NOX_FLAG\"])\n",
    "nb_air_quality_p2[\"PM25 BAM\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"PM25 BAM\"],nb_air_quality_p2[\"PM25 BAM_FLAG\"])\n",
    "nb_air_quality_p2[\"PM_2.5API\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"PM_2.5API\"],nb_air_quality_p2[\"PM_2.5API_FLAG\"])\n",
    "nb_air_quality_p2[\"Wind Dir\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"Wind Dir\"],nb_air_quality_p2[\"Wind Dir_FLAG\"])\n",
    "nb_air_quality_p2[\"Wind Speed\"] = remove_unit_when_flag_vec(nb_air_quality_p2[\"Wind Speed\"],nb_air_quality_p2[\"Wind Speed_FLAG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_flagged_value_to_info(unit_val,info_val):\n",
    "    if(str(unit_val).startswith(\"(\")):\n",
    "        unit_val = re.findall('\\((.*?)\\)',unit_val)[0]\n",
    "        \n",
    "        return info_val + \", Flagged Value: \"+ re.sub('\\((.*?)\\)','',str(unit_val)) if pd.isnull(info_val)==False else info_val\n",
    "    else:\n",
    "        return info_val \n",
    "    \n",
    "add_flagged_value_to_info_vec = np.vectorize(add_flagged_value_to_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p2[\"SO2_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"SO2\"],nb_air_quality_p2[\"SO2_INFO\"])\n",
    "nb_air_quality_p2[\"O3_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"O3\"],nb_air_quality_p2[\"O3_INFO\"])\n",
    "nb_air_quality_p2[\"CO_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"CO\"],nb_air_quality_p2[\"CO_INFO\"])\n",
    "nb_air_quality_p2[\"TRS_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"TRS\"],nb_air_quality_p2[\"TRS_INFO\"])\n",
    "nb_air_quality_p2[\"NO2_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"NO2\"],nb_air_quality_p2[\"NO2_INFO\"])\n",
    "nb_air_quality_p2[\"NO_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"NO\"],nb_air_quality_p2[\"NO_INFO\"])\n",
    "nb_air_quality_p2[\"NOX_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"NOX\"],nb_air_quality_p2[\"NOX_INFO\"])\n",
    "nb_air_quality_p2[\"PM_25_BAM_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"PM25 BAM\"],nb_air_quality_p2[\"PM_25_BAM_INFO\"])\n",
    "nb_air_quality_p2[\"PM_25_API_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"PM_2.5API\"],nb_air_quality_p2[\"PM_25_API_INFO\"])\n",
    "nb_air_quality_p2[\"WIND_DIR_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"Wind Dir\"],nb_air_quality_p2[\"WIND_DIR_INFO\"])\n",
    "nb_air_quality_p2[\"WIND_SPEED_INFO\"] = add_flagged_value_to_info_vec(nb_air_quality_p2[\"Wind Speed\"],nb_air_quality_p2[\"WIND_SPEED_INFO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_flagged_values(unit_val):\n",
    "    if(str(unit_val).startswith(\"(\")):\n",
    "        return np.nan\n",
    "    \n",
    "    return unit_val\n",
    "\n",
    "remove_flagged_values_vec = np.vectorize(remove_flagged_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec_re = np.vectorize(remove_flagged_values)\n",
    "#nb_air_quality_p2[\"SO2\"] = vec_re(nb_air_quality_p2[\"SO2\"])\n",
    "\n",
    "nb_air_quality_p2[\"SO2\"] = remove_flagged_values_vec(nb_air_quality_p2[\"SO2\"])\n",
    "nb_air_quality_p2[\"O3\"] = remove_flagged_values_vec(nb_air_quality_p2[\"O3\"])\n",
    "nb_air_quality_p2[\"CO\"] = remove_flagged_values_vec(nb_air_quality_p2[\"CO\"])\n",
    "nb_air_quality_p2[\"TRS\"] = remove_flagged_values_vec(nb_air_quality_p2[\"TRS\"])\n",
    "nb_air_quality_p2[\"NO2\"] = remove_flagged_values_vec(nb_air_quality_p2[\"NO2\"])\n",
    "nb_air_quality_p2[\"NO\"] = remove_flagged_values_vec(nb_air_quality_p2[\"NO\"])\n",
    "nb_air_quality_p2[\"NOX\"] = remove_flagged_values_vec(nb_air_quality_p2[\"NOX\"])\n",
    "nb_air_quality_p2[\"PM25 BAM\"] = remove_flagged_values_vec(nb_air_quality_p2[\"PM25 BAM\"])\n",
    "nb_air_quality_p2[\"PM_2.5API\"] = remove_flagged_values_vec(nb_air_quality_p2[\"PM_2.5API\"])\n",
    "nb_air_quality_p2[\"Wind Dir\"] = remove_flagged_values_vec(nb_air_quality_p2[\"Wind Dir\"])\n",
    "nb_air_quality_p2[\"Wind Speed\"] = remove_flagged_values_vec(nb_air_quality_p2[\"Wind Speed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p3 = nb_air_quality_p2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"SO2\", \"O3\", \"CO\", \"TRS\", \"NO2\", \"NO\", \"NOX\", \"PM25 BAM\", \"PM_2.5API\", \"Wind Dir\", \"Wind Speed\"]\n",
    "\n",
    "nb_air_quality_p3[cols] = nb_air_quality_p3[cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_one_variale(var_secondary, var_primary,var_secondary_source,var_primary_source):\n",
    "    if((var_primary == \"\" or math.isnan(var_primary)) and (var_secondary == \"\" or math.isnan(var_secondary))):\n",
    "        #print(\"Both\")\n",
    "        return \"\"\n",
    "    if(var_primary == \"\" or math.isnan(var_primary)):\n",
    "        #print(var_secondary)\n",
    "        return str(var_secondary) + \"(\"+var_secondary_source +\")\"\n",
    "    \n",
    "    return str(var_primary) + \"(\"+var_primary_source +\")\"\n",
    "\n",
    "select_one_variale_vec = np.vectorize(select_one_variale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p3[\"PM_25\"] = select_one_variale_vec(nb_air_quality_p3[\"PM25 BAM\"],nb_air_quality_p3[\"PM_2.5API\"],\"PM_25_BAM\",\"PM_25_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate source columns\n",
    "\n",
    "def separate_source_columns(raw_value):\n",
    "    #print(raw_value)\n",
    "    if(pd.isnull(raw_value)==False or math.isnan(raw_value)==False):\n",
    "        #print(raw_value)\n",
    "        source_only = re.findall('\\((.*?)\\)',raw_value)\n",
    "        source_only = source_only[0] if len(source_only) > 0 else \"\"\n",
    "        #print(source_only)\n",
    "        return source_only\n",
    "    else:\n",
    "        return \"\" \n",
    "        \n",
    "separate_source_columns_vec = np.vectorize(separate_source_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p3[\"PM_25_SOURCE\"] = separate_source_columns_vec(nb_air_quality_p3[\"PM_25\"])\n",
    "#nb_air_quality_p3[\"PM_25_SOURCE\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove source data from analytes\n",
    "\n",
    "def remove_source_name(raw_value):\n",
    "    if(pd.isnull(raw_value)==False):\n",
    "        #print(raw_value)\n",
    "        value_cleaned = re.sub('\\((.*?)\\)','',raw_value)\n",
    "        #print(value_cleaned)\n",
    "        return value_cleaned\n",
    "    else:\n",
    "        return \"\"  \n",
    "        \n",
    "remove_source_name_vec = np.vectorize(remove_source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p3[\"PM_25\"] = remove_source_name_vec(nb_air_quality_p3[\"PM_25\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PM flag columns\n",
    "\n",
    "def create_one_flag(flag_source,flag_source_to_test,flag_primary,flag_secondary):\n",
    "    if(flag_source==flag_source_to_test):\n",
    "        return flag_primary\n",
    "    return flag_secondary\n",
    "create_one_flag_vec = np.vectorize(create_one_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p3[\"PM_25_FLAG\"] = create_one_flag_vec(nb_air_quality_p3[\"PM_25_SOURCE\"],\"PM_25_API\",nb_air_quality_p3[\"PM_2.5API_FLAG\"],nb_air_quality_p3[\"PM25 BAM_FLAG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p4 = nb_air_quality_p3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns to remove whitespace\n",
    "nb_air_quality_p4.rename(columns={'PM25 BAM': 'PM_25_BAM', 'PM25 BAM_FLAG': 'PM_25_BAM_FLAG', 'PM_2.5API': 'PM_25_API', 'PM_2.5API_FLAG': 'PM_25_API_FLAG', 'Wind Dir': 'WIND_DIR', 'Wind Dir_FLAG': 'WIND_DIR_FLAG', 'Wind Speed': 'WIND_SPEED', 'Wind Speed_FLAG': 'WIND_SPEED_FLAG'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually drop `Flag` columns that doesnt have realted analyte information \n",
    "\n",
    "nb_air_quality_p4 = nb_air_quality_p4.drop(['AQHI_FLAG','AQI_FLAG','StDv_FLAG', 'PM25 TEOM_FLAG', 'PM10 TEOM_FLAG', 'GRIMM PM 1.0_FLAG', 'GRIMM PM 2.5_FLAG', 'GRIMM PM 10_FLAG', 'TEMP_FLAG', 'Rain_FLAG', 'Bar Press_FLAG', 'RH_FLAG', 'PM_10API_FLAG', 'TRS_API_FLAG', 'SO2_Run24hr_FLAG', 'H2S_FLAG', 'Battery_FLAG', 'Rain Intensity_FLAG', 'Rain Duration_FLAG', 'Wind Dir V_FLAG', 'WD_LS_FLAG', 'WS_LS_FLAG'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_p4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag columns data type to int\n",
    "\n",
    "cols = [\"SO2_FLAG\", \"O3_FLAG\", \"NO2_FLAG\", \"NO_FLAG\", \"NOX_FLAG\", \"PM_25_BAM_FLAG\", \"PM_25_API_FLAG\", \"TRS_FLAG\", \"CO_FLAG\", \"WIND_DIR_FLAG\", \"WIND_SPEED_FLAG\"]\n",
    "\n",
    "nb_air_quality_p4[cols] = nb_air_quality_p4[cols].fillna(-1)\n",
    "nb_air_quality_p4[cols] = nb_air_quality_p4[cols].astype('int32')\n",
    "nb_air_quality_p4[cols] = nb_air_quality_p4[cols].replace(-1, np.nan, regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty String to NaN\n",
    "\n",
    "cols = ['PM_25','PM_25_SOURCE']\n",
    "nb_air_quality_p4[cols] = nb_air_quality_p4[cols].replace(\"\",np.nan)\n",
    "\n",
    "nb_air_quality_p4['PM_25'] = nb_air_quality_p4['PM_25'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round the Coulmns to 1 decimal point\n",
    "\n",
    "#cols = ['SO2', 'O3','NO2','NO', 'NOX','PM_25_BAM','TRS', 'WIND_DIR','WIND_SPEED', 'PM_25_API', 'CO']\n",
    "\n",
    "#nb_air_quality_p4[cols] = nb_air_quality_p4[cols].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange columns\n",
    "\n",
    "cols = ['STATION_ID', 'DATE_TIME','DATE', 'YEAR', 'STATION_NAME', 'LATITUDE', 'LONGITUDE', 'STATION_NAPS_ID', 'ORGANIZATION', 'CITY', 'SO2', 'SO2_FLAG', 'O3', 'O3_FLAG', 'NO2', 'NO2_FLAG', 'NO', 'NO_FLAG', 'NOX', 'NOX_FLAG','PM_25','PM_25_FLAG', 'PM_25_SOURCE', 'TRS', 'TRS_FLAG', 'CO', 'CO_FLAG', 'WIND_DIR', 'WIND_DIR_FLAG', 'WIND_SPEED', 'WIND_SPEED_FLAG']\n",
    "\n",
    "nb_air_quality_p4 = nb_air_quality_p4[cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a final copy of processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality = nb_air_quality_p4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Combined Dataset to a CSV\n",
    "\n",
    "nb_air_quality.to_csv(\"data/nb_air_quality.csv\", sep=',',index=False,encoding='utf-8-sig')\n",
    "\n",
    "#Shape of row data\n",
    "nb_air_quality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot null values\n",
    "def plot_null_values(dataset,group_by):\n",
    "\n",
    "    dataset = dataset.groupby([group_by])\n",
    "    # extract keys from groups\n",
    "    keys = dataset.groups.keys()\n",
    "\n",
    "    totalCols=3\n",
    "    totalRows=math.ceil(len(dataset)/totalCols)\n",
    "    \n",
    "    fig = plt.figure(figsize=((totalCols+3)*3,(totalRows+1)*5))\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.7)\n",
    "\n",
    "    for index, x in enumerate(keys):\n",
    "        null_columns = []\n",
    "        null_column_values = []\n",
    "\n",
    "        #print(dataset.get_group(x).columns[2])\n",
    "\n",
    "        for i in range(0,len(dataset.get_group(x).columns),1):\n",
    "            if(dataset[dataset.get_group(x).columns[i]].get_group(x).isna().sum() > 0):\n",
    "                null_columns.append(dataset.get_group(x).columns[i])\n",
    "                null_column_values.append(dataset[dataset.get_group(x).columns[i]].get_group(x).isna().sum())\n",
    "\n",
    "        globals()[f\"ax_count_plots_{index}\"] = fig.add_subplot(totalRows,totalCols,(index+1))\n",
    "        globals()[f\"ax_count_plots_{index}\"].set_title(x.upper())\n",
    "        \n",
    "\n",
    "        globals()[f\"ax_count_plots_{index}\"].bar(null_columns,null_column_values)\n",
    "        \n",
    "        globals()[f\"ax_count_plots_{index}\"].set(xlabel=None)\n",
    "        globals()[f\"ax_count_plots_{index}\"].tick_params(axis='x', labelrotation = 90)\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_null_values(nb_air_quality,'STATION_NAME')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This is just an example of a line graph, visualization can be better\n",
    "def linechart_of_categories(dataset,group_by,time_column,value_column):\n",
    "\n",
    "    dataset = dataset.set_index(time_column)\n",
    "    dataset = dataset.groupby([group_by])\n",
    "    # extract keys from groups\n",
    "    keys = dataset.groups.keys()\n",
    "    totalRows = 0\n",
    "    for index, x in enumerate(keys):\n",
    "        if(len(dataset[value_column].get_group(x))!=dataset[value_column].get_group(x).isna().sum()):\n",
    "            totalRows+=1\n",
    "    \n",
    "    totalCols=3\n",
    "    totalRows=math.ceil(totalRows/totalCols)\n",
    "    \n",
    "    fig = plt.figure(figsize=((totalCols+3)*3,(totalRows+1)*5))\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.6)\n",
    "    newInx = 1\n",
    "    for index, x in enumerate(keys):\n",
    "        if(len(dataset[value_column].get_group(x))!=dataset[value_column].get_group(x).isna().sum()):\n",
    "            globals()[f\"ax_count_plots_{index}\"] = fig.add_subplot(totalRows,totalCols,newInx)\n",
    "            globals()[f\"ax_count_plots_{index}\"].set_title(x.upper())\n",
    "            #if(len(dataset[value_column].get_group(x))!=dataset[value_column].get_group(x).isna().sum()):\n",
    "            dataset[value_column].get_group(x).plot()\n",
    "            \n",
    "            globals()[f\"ax_count_plots_{index}\"].set(xlabel=None)\n",
    "            globals()[f\"ax_count_plots_{index}\"].tick_params(axis='x', labelrotation = 90)\n",
    "            newInx+=1\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linechart_of_categories(nb_air_quality,'STATION_NAME','DATE','PM_25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display unique values\n",
    "\n",
    "def unique_values__or_count(listOfColumns,options,dataset):\n",
    "    for x in range(0, len(listOfColumns), 1):\n",
    "        if(options==\"unique\"):\n",
    "            unique_values_str = dataset[listOfColumns[x]].unique()\n",
    "            print(\"unique_values \" + listOfColumns[x])\n",
    "            print(unique_values_str)\n",
    "            print(\"------------------------\")\n",
    "        if(options==\"count\"):\n",
    "            values_distribution = dataset[listOfColumns[x]].value_counts()\n",
    "            print(\"-----------\"+listOfColumns[x] +\"------------\")\n",
    "            print(values_distribution)\n",
    "            print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values__or_count(['SO2_FLAG', 'O3_FLAG', 'NO2_FLAG', 'NO_FLAG', 'NOX_FLAG', 'PM_25_FLAG', 'TRS_FLAG', 'CO_FLAG', 'WIND_DIR_FLAG', 'WIND_SPEED_FLAG'],\"count\",nb_air_quality)\n",
    "\n",
    "#83,85,89,78,86,27,77,22"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import cleaned data \n",
    "\n",
    "nb_air_quality_validate = pd.read_csv(\"data/nb_air_quality.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Col_name_to_validate = \"NO2\"\n",
    "flag_name = Col_name_to_validate+\"_FLAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_validate[\"DATE_TIME\"] = pd.to_datetime(nb_air_quality_validate[\"DATE_TIME\"],infer_datetime_format=True, format='%Y-%m-%d %H:%M:%S')\n",
    "nb_acid_rain_validate = nb_air_quality_validate[[\"STATION_ID\", \"DATE_TIME\", Col_name_to_validate,flag_name]].copy()\n",
    "\n",
    "nb_acid_rain_validate.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import original data\n",
    "\n",
    "nb_air_quality_original = pd.read_csv(\"raw_data/28_04_2023 09_43132.lsi\", low_memory=False, header=None,sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_original.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Col_index_in_original = 6\n",
    "\n",
    "#Rename columns to remove whitespace\n",
    "nb_air_quality_original.rename(columns={0: 'STATION_ID', 1: 'DATE_TIME', Col_index_in_original: Col_name_to_validate, Col_index_in_original+1: flag_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_braces(unit_val):\n",
    "    if(str(unit_val).startswith(\"(\")):\n",
    "        unit_val = re.findall('\\((.*?)\\)',unit_val)[0]\n",
    "        return unit_val\n",
    "    \n",
    "    return unit_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -9999 with NaN\n",
    "nb_air_quality_original = nb_air_quality_original.replace(-9999,np.nan)\n",
    "\n",
    "#Change date format\n",
    "nb_air_quality_original[\"DATE_TIME\"] = pd.to_datetime(nb_air_quality_original[\"DATE_TIME\"],infer_datetime_format=True, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#Round to 1 decimal\n",
    "nb_air_quality_original[Col_name_to_validate] = nb_air_quality_original[Col_name_to_validate].round(1)\n",
    "\n",
    "#Trim data to validate an analyte\n",
    "nb_air_quality_original = nb_air_quality_original[[\"STATION_ID\", \"DATE_TIME\", Col_name_to_validate,flag_name]].copy()\n",
    "\n",
    "#Remove flagged\n",
    "nb_air_quality_original[Col_name_to_validate] = nb_air_quality_original.apply(lambda x: remove_unit_when_flag(x[Col_name_to_validate],x[flag_name]),axis=1)\n",
    "\n",
    "#Remove flagged values\n",
    "nb_air_quality_original[Col_name_to_validate] = nb_air_quality_original.apply(lambda x: remove_flagged_values(x[Col_name_to_validate]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_original.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_air_quality_validate_results = pd.merge(nb_acid_rain_validate, nb_air_quality_original, on=[\"STATION_ID\", \"DATE_TIME\",Col_name_to_validate,flag_name], how='right', indicator='Exist')\n",
    "nb_air_quality_validate_results['Exist'] = np.where(nb_air_quality_validate_results.Exist == 'both', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values__or_count(['Exist'],\"count\",nb_air_quality_validate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_missing_rows = nb_air_quality_validate_results[(nb_air_quality_validate_results['Exist'] == False)].copy()\n",
    "\n",
    "list_of_missing_rows.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c481cd3fc83eb5588546703e9d94f1058347710cd104ca041365c752440aa931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
