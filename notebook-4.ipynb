{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakes Program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import re\n",
    "#import openpyxl\n",
    "#import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSV files from a folder\n",
    "\n",
    "current_directory = str(os.getcwd()) + \"\\\\raw_data\\\\\"\n",
    "dataframes = []\n",
    "\n",
    "all_files = []\n",
    "\n",
    "for path, subdirs, files in os.walk(current_directory):\n",
    "    for name in files:\n",
    "        file_name = os.path.join(path, name)\n",
    "        format_matches = [\".csv\"]\n",
    "        exclue_matches = []\n",
    "        if name not in all_files:\n",
    "            if any([x in name for x in format_matches]):\n",
    "                if not any([y in file_name for y in exclue_matches]):\n",
    "                    try:\n",
    "                        current_dataframe = pd.read_csv(file_name, low_memory=False,sep=\",\")\n",
    "                        dataframes.append(current_dataframe)\n",
    "\n",
    "                        pass\n",
    "                    except Exception as e:\n",
    "                        print(\"Error reading file: \" + file_name)\n",
    "                        print(e)\n",
    "                else:\n",
    "                    print(\"Files Excluded : \" + file_name)\n",
    "            else:\n",
    "                print(\"Non Excel File: \" + file_name)\n",
    "        \n",
    "        all_files.append(name)\n",
    "all_files = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if dataframes have the same columns\n",
    "\n",
    "if all([set(dataframes[0].columns) == set(df.columns) for df in dataframes]):\n",
    "    print('Datasets have the same columns')\n",
    "else:\n",
    "    print('Datasets do not have the same columns')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the columns names that have found in some columns but not in others. This way we can create those columns for all the dataframes\n",
    "\n",
    "columns = []\n",
    "    \n",
    "for x in range(0, len(dataframes), 1):\n",
    "    for y in range(0, len(dataframes), 1):\n",
    "        for z in range(0, len(dataframes[x].columns), 1):\n",
    "            #print(str(z) + \"||\"+ str(len(dataframes[y].columns))+ \"||\" + str(y))\n",
    "            if(dataframes[x].columns[z] in dataframes[y].columns):\n",
    "                pass\n",
    "            else:\n",
    "                if (dataframes[x].columns[z] in columns):\n",
    "                    pass\n",
    "                else:\n",
    "                    columns.append(dataframes[x].columns[z])\n",
    "                \n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the dataframes into one\n",
    "\n",
    "lakes_program_raw = pd.concat(dataframes)\n",
    "lakes_program_raw.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Combined Dataset to a CSV\n",
    "\n",
    "lakes_program_raw.to_csv(\"data/lakes_program_raw.csv\", sep=',',index=False,encoding='utf-8-sig')\n",
    "\n",
    "#Shape of row data\n",
    "lakes_program_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of the dataset\n",
    "\n",
    "lakes_program_p1 = lakes_program_raw.copy()\n",
    "lakes_program_p1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -9999 with NaN\n",
    "#nb_air_quality_p1 = nb_air_quality_p1.replace(-9999,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datetime_format(dt_str):\n",
    "    formats_to_check = [\n",
    "        '%Y/%m/%d %I:%M:%S %p',\n",
    "        '%Y-%m-%d %I:%M:%S %p',\n",
    "        '%Y-%m-%d %H:%M:%S.%f %p',\n",
    "        '%Y/%m/%d %I.%M.%S.%f %p',\n",
    "        '%Y/%m/%d %H:%M:%S',\n",
    "        '%Y-%m-%d %H:%M:%S',\n",
    "        '%d/%m/%Y %I:%M:%S %p',\n",
    "        '%d-%m-%Y %I:%M:%S %p',\n",
    "        '%d/%m/%Y %H:%M:%S',\n",
    "        '%d-%m-%Y %H:%M:%S',\n",
    "        '%Y/%m/%d',\n",
    "        '%Y-%m-%d',\n",
    "        '%d/%m/%Y',\n",
    "        '%d-%m-%Y',\n",
    "    ]\n",
    "\n",
    "    for fmt in formats_to_check:\n",
    "        try:\n",
    "            datetime.datetime.strptime(dt_str, fmt)\n",
    "            return fmt\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "dt_str = \"2003-06-10 13:00:00\"\n",
    "format_found = find_datetime_format(dt_str)\n",
    "if format_found:\n",
    "    print(f\"Format found: {format_found}\")\n",
    "else:\n",
    "    print(\"Format not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the date format\n",
    "lakes_program_p1[\"RESULTDATE\"] = pd.to_datetime(lakes_program_p1[\"RESULTDATE\"],format='%Y/%m/%d %I.%M.%S.%f %p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p1[\"PARMABBR\"] = lakes_program_p1[\"PARMABBR\"].replace('NH3,Un-ion','NH3_Un-ion' , regex=True)\n",
    "lakes_program_p1[\"PARMABBR\"] = lakes_program_p1[\"PARMABBR\"].replace('CHL\"A\"','CHL_A' , regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lakes_program_p1_f1 = lakes_program_p1[(lakes_program_p1['RESULTDEPTH'].isnull() == False)]\n",
    "#lakes_program_p1_f2 = lakes_program_p1[(lakes_program_p1['RESULTDEPTH'].isnull() == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lakes_program_p1_f1.shape)\n",
    "#print(lakes_program_p1_f2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_analyte_columns = lakes_program_p1[\"RESULTDATE\"].unique()\n",
    "print(len(new_analyte_columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and remove null columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_columns_dataset(dataset):\n",
    "    indexes = []\n",
    "    for i in range(0,len(dataset.columns),1):\n",
    "        if(len(dataset[dataset.columns[i]])==dataset[dataset.columns[i]].isna().sum()):\n",
    "            indexes.append(dataset.columns[i])\n",
    "            print(dataset.columns[i])\n",
    "   \n",
    "    dataset.drop(indexes,inplace=True, axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p1 = drop_empty_columns_dataset(lakes_program_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p1 = lakes_program_p1.replace({None: np.nan})\n",
    "#lakes_program_p1 = lakes_program_p1.replace({np.nan: \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p1.shape\n",
    "#lakes_program_p1.iloc[:, [3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"RESULTID\",\"PARMABBR\",\"RESULTVAL\",\"PARMFLAG\",\"STATIONID\",\"PARMSRCDESCE\",\"RESULTDEPTH\"]\n",
    "#cols = [\"RESULT_ID_DEPTH\",\"PARMABBR\",\"RESULTVAL\"]\n",
    "lakes_program_p1_a = lakes_program_p1[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def find_unique_values_in_groups_with_depth(lakes_program_p1_a):\n",
    "    surface_water_monitoring_groups = lakes_program_p1_a.groupby([\"PARMABBR\"])\n",
    "    lakes_dataframes_grouped = []\n",
    "    #dataset = dataset.set_index(time_column)\n",
    "\n",
    "    # extract keys from groups\n",
    "    keys = surface_water_monitoring_groups.groups.keys()\n",
    "    #indexnew = 0\n",
    "    for index, x in enumerate(keys):\n",
    "        #if(indexnew < 2000):\n",
    "        current_dataframe = surface_water_monitoring_groups.get_group(x)\n",
    "        column_name = current_dataframe[\"PARMABBR\"].unique()[0]\n",
    "        flag_name = current_dataframe[\"PARMABBR\"].unique()[0] + \"_FLAG\"\n",
    "        current_dataframe = current_dataframe.rename(columns={'RESULTVAL': column_name,'PARMFLAG': flag_name,'PARMSRCDESCE': column_name+\"_SOURCE\",'RESULTDEPTH':column_name+\"_DEPTH\"})\n",
    "        current_dataframe = current_dataframe.drop('PARMABBR', axis=1)\n",
    "        #indexnew+=1\n",
    "\n",
    "        cols = [\"RESULTID\",\"RESULTDATE\",\"PARMABBR\",\"STATIONID\"]\n",
    "        current_dataframe_date_data = lakes_program_p1[cols].copy()\n",
    "        current_dataframe_date_data = current_dataframe_date_data[(current_dataframe_date_data[\"PARMABBR\"]==x)]\n",
    "        current_dataframe_date_data = current_dataframe_date_data.drop('STATIONID', axis=1)\n",
    "\n",
    "        current_dataframe = pd.merge(current_dataframe, current_dataframe_date_data, how='left', left_on=[\"RESULTID\"], right_on = [\"RESULTID\"])\n",
    "        current_dataframe = current_dataframe.sort_values(by='RESULTDATE')\n",
    "        current_dataframe[\"DATE_COUNT\"] = current_dataframe.groupby(['RESULTDATE','STATIONID']).cumcount() + 1\n",
    "        current_dataframe[\"DATE_COUNT\"] = current_dataframe.RESULTDATE.astype(str) +\"(\"+ current_dataframe.DATE_COUNT.astype(str) +\")[\"+ current_dataframe.STATIONID.astype(str)+\"]\"\n",
    "\n",
    "        drop_cols = ['PARMABBR','RESULTID','RESULTDATE','STATIONID']\n",
    "        current_dataframe = current_dataframe.drop(drop_cols, axis=1)\n",
    "\n",
    "        current_dataframe = current_dataframe.dropna(subset=[column_name,flag_name], how='all')\n",
    "        lakes_dataframes_grouped.append(current_dataframe)\n",
    "        #current_dataframe.to_csv(\"data/temp/lakes-program-\"+x+\".csv\", sep=',',index=False,encoding='utf-8-sig')\n",
    "    \n",
    "    return lakes_dataframes_grouped\n",
    "        \n",
    "lakes_dataframes_grouped = find_unique_values_in_groups_with_depth(lakes_program_p1_a)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_values_in_groups(lakes_program_p1_a):\n",
    "    surface_water_monitoring_groups = lakes_program_p1_a.groupby([\"PARMABBR\"])\n",
    "    lakes_dataframes_grouped = []\n",
    "    #dataset = dataset.set_index(time_column)\n",
    "\n",
    "    # extract keys from groups\n",
    "    keys = surface_water_monitoring_groups.groups.keys()\n",
    "    #indexnew = 0\n",
    "    for index, x in enumerate(keys):\n",
    "        #if(indexnew < 2000):\n",
    "        current_dataframe = surface_water_monitoring_groups.get_group(x)\n",
    "        column_name = current_dataframe[\"PARMABBR\"].unique()[0]\n",
    "        flag_name = current_dataframe[\"PARMABBR\"].unique()[0] + \"_FLAG\"\n",
    "        current_dataframe = current_dataframe.rename(columns={'RESULTVAL': column_name,'PARMFLAG': flag_name,'PARMSRCDESCE': column_name+\"_SOURCE\"})\n",
    "        current_dataframe = current_dataframe.drop('PARMABBR', axis=1)\n",
    "        #indexnew+=1\n",
    "\n",
    "        cols = [\"RESULTID\",\"RESULTDATE\",\"PARMABBR\",\"STATIONID\"]\n",
    "        current_dataframe_date_data = lakes_program_p1[cols].copy()\n",
    "        current_dataframe_date_data = current_dataframe_date_data[(current_dataframe_date_data[\"PARMABBR\"]==x)]\n",
    "        current_dataframe_date_data = current_dataframe_date_data.drop('STATIONID', axis=1)\n",
    "\n",
    "        current_dataframe = pd.merge(current_dataframe, current_dataframe_date_data, how='left', left_on=[\"RESULTID\"], right_on = [\"RESULTID\"])\n",
    "        current_dataframe = current_dataframe.sort_values(by='RESULTDATE')\n",
    "        current_dataframe[\"DATE_COUNT\"] = current_dataframe.groupby(['RESULTDATE','STATIONID']).cumcount() + 1\n",
    "        current_dataframe[\"DATE_COUNT\"] = current_dataframe.RESULTDATE.astype(str) +\"(\"+ current_dataframe.DATE_COUNT.astype(str) +\")[\"+ current_dataframe.STATIONID.astype(str)+\"]{\"+ current_dataframe.RESULTDEPTH.astype(str)+\"}\"\n",
    "\n",
    "        drop_cols = ['PARMABBR','RESULTID','RESULTDATE','STATIONID',\"RESULTDEPTH\"]\n",
    "        current_dataframe = current_dataframe.drop(drop_cols, axis=1)\n",
    "\n",
    "        current_dataframe = current_dataframe.dropna(subset=[column_name,flag_name], how='all')\n",
    "        lakes_dataframes_grouped.append(current_dataframe)\n",
    "        #current_dataframe.to_csv(\"data/temp/lakes-program-\"+x+\".csv\", sep=',',index=False,encoding='utf-8-sig')\n",
    "    \n",
    "    return lakes_dataframes_grouped\n",
    "        \n",
    "lakes_dataframes_grouped = find_unique_values_in_groups(lakes_program_p1_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_dataframes_grouped[6].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe with unique values of datecount column \n",
    "\n",
    "date_count_dataframe = pd.concat(lakes_dataframes_grouped)\n",
    "lakes_empty_df = pd.DataFrame({'DATE_COUNT':date_count_dataframe[\"DATE_COUNT\"].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dataframe, choose one of the dataframes to start the merging process\n",
    "lakes_merged_df = lakes_empty_df.copy()\n",
    "\n",
    "# Loop through the rest of the dataframes and merge\n",
    "for df in lakes_dataframes_grouped:\n",
    "    #print(len(df))\n",
    "    lakes_merged_df = pd.merge(lakes_merged_df, df, how='left', left_on=[\"DATE_COUNT\"], right_on = [\"DATE_COUNT\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_merged_df= lakes_merged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create another copy of the dataset for futher pre-processing\n",
    "\n",
    "Some methods are slow when processing data. Creating a copy of a dataset will allow us not to run the entire code during data development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy dataset to new variable\n",
    "\n",
    "lakes_program_p2 = lakes_merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p2.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate Station ID columns\n",
    "\n",
    "def separate_data_from_date_column(raw_value,values_to_find):\n",
    "    #print(raw_value)\n",
    "    source_only = \"\"\n",
    "    if(values_to_find==\"RESULT_COUNT\"):\n",
    "        #print(raw_value)\n",
    "        source_only = re.findall('\\((.*?)\\)',raw_value)\n",
    "    elif(values_to_find==\"STATION_ID\"):\n",
    "        source_only = re.findall('\\[(.*?)\\]',raw_value)\n",
    "    elif(values_to_find==\"DEPTH_M\"):\n",
    "        source_only = re.findall('\\{(.*?)\\}',raw_value) \n",
    "    source_only = source_only[0] if len(source_only) > 0 else source_only\n",
    "    #print(source_only)\n",
    "    return source_only\n",
    "        \n",
    "separate_data_from_date_column_vec = np.vectorize(separate_data_from_date_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns by extrating information from DATE_COUNT\n",
    "\n",
    "lakes_program_p2[\"RESULT_COUNT\"] = separate_data_from_date_column_vec(lakes_program_p2[\"DATE_COUNT\"],\"RESULT_COUNT\")\n",
    "lakes_program_p2[\"STATION_ID\"] = separate_data_from_date_column_vec(lakes_program_p2[\"DATE_COUNT\"],\"STATION_ID\")\n",
    "lakes_program_p2[\"DEPTH_M\"] = separate_data_from_date_column_vec(lakes_program_p2[\"DATE_COUNT\"],\"DEPTH_M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove data from date column\n",
    "\n",
    "def remove_extra_data_from_date_column(raw_value):\n",
    "    if(pd.isnull(raw_value)==False):\n",
    "        #print(raw_value)\n",
    "        value_cleaned = re.sub('\\((.*?)\\)','',raw_value)\n",
    "        value_cleaned = re.sub('\\[(.*?)\\]','',value_cleaned)\n",
    "        value_cleaned = re.sub('\\{(.*?)\\}','',value_cleaned)\n",
    "        #print(value_cleaned)\n",
    "        return value_cleaned\n",
    "    else:\n",
    "        return \"\"  \n",
    "        \n",
    "remove_extra_data_from_date_column_vec = np.vectorize(remove_extra_data_from_date_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p2[\"DATE_COUNT\"] = remove_extra_data_from_date_column_vec(lakes_program_p2[\"DATE_COUNT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change date format\n",
    "\n",
    "lakes_program_p2[\"DATE_COUNT\"] = pd.to_datetime(lakes_program_p2[\"DATE_COUNT\"],format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#Create Year Column\n",
    "\n",
    "lakes_program_p2[\"YEAR\"] = lakes_program_p2[\"DATE_COUNT\"].dt.year\n",
    "\n",
    "#Rename columns before station informtion\n",
    "lakes_program_p2.rename(columns={'DATE_COUNT': 'DATE_TIME'}, inplace=True)\n",
    "\n",
    "lakes_program_p2.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Station information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the data type for Sation id column\n",
    "\n",
    "lakes_program_p2[\"STATION_ID\"] =lakes_program_p2['STATION_ID'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data \n",
    "\n",
    "station_information = pd.read_csv(\"data/lakes-program-stations.csv\")\n",
    "\n",
    "#station_information[\"MOST_RECENT_SAMPLE_DATE\"] = pd.to_datetime(station_information[\"MOST_RECENT_SAMPLE_DATE\"],format='%Y/%m/%d %I.%M.%S.%f00000000 %p')\n",
    "\n",
    "station_information[\"DMS_LATITUDE\"] = np.nan\n",
    "station_information[\"DMS_LONGITUDE\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DMS (degrees, minutes, seconds) to DD (decimal degrees)\n",
    "def dms2dd(degrees, minutes, seconds, direction):\n",
    "    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60)\n",
    "    if direction == 'S' or direction == 'W':\n",
    "        dd *= -1\n",
    "    return dd\n",
    "\n",
    "def dd2dms(dms,dd, pre_fix_latlong):\n",
    "    if(pd.isnull(dms) == True and pd.isnull(dd) == False):\n",
    "        d = int(dd)\n",
    "        md = abs(dd - d) * 60\n",
    "        m = int(md)\n",
    "        sd = (md - m) * 60\n",
    "        #return [d, m, sd] \n",
    "        #print(\"%s %s˚ %s' %s\\\"\" % (pre_fix_latlong,abs(d),m,round(sd,1)))\n",
    "        return \"%s %s˚ %s' %s\\\"\" % (pre_fix_latlong,abs(d),m,round(sd,1))\n",
    "    return dms\n",
    "\n",
    "def parse_dms(dms,latlong):\n",
    "    if(pd.isnull(dms) != True):\n",
    "        #print(dms)\n",
    "        dms=dms.replace('\"','')\n",
    "        degDirection, minutes, seconds = re.split('[˚\\']', dms)\n",
    "        direction,deg = re.split('[\\s]', degDirection)\n",
    "        #print(deg, minutes, seconds, direction)\n",
    "        latLng = dms2dd(deg, minutes, seconds, direction)\n",
    "\n",
    "        return (latLng)\n",
    "    else:\n",
    "        return latlong\n",
    "\n",
    "#dd = parse_dms(\"36°57'9' N 110°4'21' W\")\n",
    "\n",
    "#print(parse_dms(\"W 67˚ 44' 01.3\",np.nan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_information[\"DMS_LATITUDE\"] = station_information.apply(lambda x: dd2dms(x[\"DMS_LATITUDE\"],x[\"LATITUDE\"],\"N\"),axis=1)\n",
    "station_information[\"DMS_LONGITUDE\"] = station_information.apply(lambda x: dd2dms(x[\"DMS_LONGITUDE\"],x[\"LONGITUDE\"],\"W\"),axis=1)\n",
    "\n",
    "station_information[\"LATITUDE\"] = station_information.apply(lambda x: parse_dms(x[\"DMS_LATITUDE\"],x[\"LATITUDE\"]),axis=1)\n",
    "station_information[\"LONGITUDE\"] = station_information.apply(lambda x: parse_dms(x[\"DMS_LONGITUDE\"],x[\"LONGITUDE\"]),axis=1)\n",
    "\n",
    "station_information = station_information.drop_duplicates()\n",
    "\n",
    "#Export stations to a CSV\n",
    "\n",
    "station_information.to_csv(\"data/lakes-program-stations.csv\", sep=',',index=False,encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attach station information\n",
    "\n",
    "lakes_program_p2 = pd.merge(lakes_program_p2, station_information[[\"STATION_ID\",\"STATION_NAME\",\"LATITUDE\",\"LONGITUDE\"]],  how='left', left_on=['STATION_ID'], right_on = ['STATION_ID'])\n",
    "\n",
    "lakes_program_p2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lakes_program_p2 = lakes_program_p2.convert_dtypes()\n",
    "#lakes_program_p2[\"RESULTVAL\"] = lakes_program_p2[\"RESULTVAL\"].astype(\"string\", errors=\"ignore\")\n",
    "#lakes_program_p2[\"DO\"] = lakes_program_p2[\"DO\"].str.strip()\n",
    "#lakes_program_p2 = lakes_program_p2.replace({\"\": np.nan})\n",
    "#lakes_program_p2= lakes_program_p2.fillna(np.nan)\n",
    "#lakes_program_p2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -9999 with NaN\n",
    "#nb_surface_water_monitoring_p1 = nb_surface_water_monitoring_p1.replace(-9999,np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "pd.set_option('display.max_rows',None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "lakes_program_p2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "#pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_p2 = drop_empty_columns_dataset(lakes_program_p2) #finish this later when we have whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lakes_program_p2.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreational_beach_monitoring_p2.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(column_name):\n",
    "    unit_only = re.findall('\\((.*?)\\)',column_name)\n",
    "    unit_only = unit_only[0] if len(unit_only) > 0 else unit_only\n",
    "    column_name_cleaned = re.sub('\\((.*?)\\)','',column_name)\n",
    "    column_name_cleaned = column_name_cleaned.replace(\" - \", \"-\")\n",
    "    column_name_cleaned = column_name_cleaned.replace(\"  \", \"_\")\n",
    "    column_name_cleaned = column_name_cleaned.replace(\" \", \"_\")\n",
    "    column_name_cleaned = column_name_cleaned.replace(\"-\", \"_\")\n",
    "    column_name_cleaned = column_name_cleaned.replace(\".\", \"_\")\n",
    "    column_name_cleaned = column_name_cleaned.replace(\",\", \"_\")\n",
    "    column_name_cleaned = column_name_cleaned.upper()\n",
    "    return [column_name_cleaned, unit_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename all columns\n",
    "\n",
    "lakes_program_p2 = lakes_program_p2.rename(columns=lambda x: clean_column_names(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get units from columns and store in a dataframe\n",
    "\n",
    "lakes_program_units = lakes_program_p1.copy()\n",
    "\n",
    "cols = [\"PARMCD\",\"PARMABBR\",\"PARMDESCE\",\"PARMDESCF\",\"UNITDESCE\",\"UNITDESCF\"]\n",
    "\n",
    "lakes_program_units = lakes_program_units[cols]\n",
    "\n",
    "lakes_program_units = lakes_program_units.drop_duplicates()\n",
    "\n",
    "lakes_program_units = lakes_program_units.rename(columns={\"PARMCD\":\"UNIT_ID\",\"PARMABBR\":\"UNIT_NAME\",\"PARMDESCE\":\"UNIT_DESC_EN\",\"PARMDESCF\":\"UNIT_DESC_FR\",\"UNITDESCE\":\"UNIT\",\"UNITDESCF\":\"UNIT_FR\"})\n",
    "\n",
    "lakes_program_units = lakes_program_units.dropna(subset = [\"UNIT_NAME\"])\n",
    "\n",
    "#Export Combined Dataset to a CSV\n",
    "\n",
    "lakes_program_units.to_csv(\"data/lakes-program-units.csv\", sep=',',index=False,encoding='utf-8-sig')\n",
    "\n",
    "lakes_program_units.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get flags from columns and store in a dataframe\n",
    "lakes_program_flags = lakes_program_p1.copy()\n",
    "\n",
    "cols = [\"PARMFLAG\",\"PARMFLAG_DESC_E\",\"PARMFLAG_DESC_F\"]\n",
    "\n",
    "lakes_program_flags = lakes_program_flags[cols]\n",
    "\n",
    "lakes_program_flags = lakes_program_flags.drop_duplicates()\n",
    "\n",
    "lakes_program_flags = lakes_program_flags.rename(columns={\"PARMFLAG\":\"FLAG_VALUE\",\"PARMFLAG_DESC_E\":\"FLAG_DESC_EN\",\"PARMFLAG_DESC_F\":\"FLAG_DESC_FR\"})\n",
    "\n",
    "lakes_program_flags = lakes_program_flags.dropna(subset = [\"FLAG_VALUE\"])\n",
    "#Export Combined Dataset to a CSV\n",
    "\n",
    "lakes_program_flags.to_csv(\"data/lakes-program-flags.csv\", sep=',',index=False,encoding='utf-8-sig')\n",
    "\n",
    "lakes_program_flags.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually rename calculated variables\n",
    "\n",
    "lakes_program_p2 = lakes_program_p2.rename(columns={\"Þ_=TDS\":\"TDS_CALC\",\"Þ_=TDS_SOURCE\":\"TDS_CALC_SOURCE\"})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round the Coulmns to 1 decimal point\n",
    "\n",
    "#cols = ['AL_ENV_LAB', 'ALK_G_ENV_LAB']\n",
    "\n",
    "#recreational_beach_monitoring_p2[cols] = recreational_beach_monitoring_p2[cols].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lakes_program_p2 = lakes_program_p2.astype(\"string\", errors=\"ignore\")\n",
    "lakes_program_p2.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of data\n",
    "\n",
    "lakes_program_p3 = lakes_program_p2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the datatype for columns\n",
    "\n",
    "lakes_program_p3[\"LATITUDE\"] =lakes_program_p3['LATITUDE'].astype(str)\n",
    "lakes_program_p3[\"LONGITUDE\"] =lakes_program_p3['LONGITUDE'].astype(str)\n",
    "lakes_program_p3[\"RESULT_COUNT\"] =lakes_program_p3['RESULT_COUNT'].astype(np.int64)\n",
    "\n",
    "lakes_program_p3[\"DEPTH_M\"] = lakes_program_p3[\"DEPTH_M\"].replace({\"nan\": np.nan})\n",
    "#lakes_program_p3[\"DEPTH_M\"] =lakes_program_p3['DEPTH_M'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if depth columns can be merged\n",
    "def check_null_groups(arr_values_analytes,dateval,station_id,RESULT_COUNT):\n",
    "    \n",
    "    checknull = 0\n",
    "    analyte_val = np.nan\n",
    "    \n",
    "    for i in range(len(arr_values_analytes)):\n",
    "        if(str(arr_values_analytes[i]).strip() == \"\" or pd.isnull(arr_values_analytes[i])==True):\n",
    "            checknull += 1\n",
    "        else:\n",
    "            analyte_val =  arr_values_analytes[i]\n",
    "    \n",
    "    if(len(arr_values_analytes)-checknull == 0):\n",
    "        #print(\"null group\" + str(dateval) +\"||\" + str(station_id) +\"||\" + str(RESULT_COUNT))\n",
    "        pass\n",
    "    elif(len(arr_values_analytes)-checknull == 1):\n",
    "        #print(str(analyte_val))\n",
    "        pass\n",
    "    else:\n",
    "        print(\"issue in group\" + str(dateval) +\"||\" + str(station_id) +\"||\" + str(RESULT_COUNT))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lakes_program_p3.apply(lambda x: check_null_groups([x['COND_DEPTH'],x['DO_DEPTH'],x['TEMP_DEPTH'],x['PH_DEPTH']],x[\"DATE_TIME\"],x[\"STATION_ID\"],x[\"RESULT_COUNT\"]),axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a final copy of processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program = lakes_program_p3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Missing Flag Columns\n",
    "\n",
    "#lakes_program_p3['AIR_TEMP_FIELD_FLAG'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = ['STATION_ID','RESULT_ID_DEPTH', 'DATE_TIME', 'YEAR', 'STATION_NAME', 'LATITUDE', 'LONGITUDE', 'ALK_G', 'ALK_T', 'AG', 'AG_FLAG', 'AL', 'AS', 'AS_FLAG', 'B', 'B_FLAG', 'BE_X', 'BE_X_FLAG', 'BR2', 'BR2_FLAG', 'BA', 'BA_FLAG', 'BI', 'BI_FLAG', 'BR', 'BR_FLAG', 'CHL_A', 'CHL_A_FLAG', 'CLRA', 'CLRA_FLAG', 'CLRT', 'CLRT_FLAG', 'COND', 'CA', 'CD', 'CD_FLAG', 'CL', 'CO', 'CO_FLAG', 'CR', 'CR_FLAG', 'CU', 'CU_FLAG', 'DO', 'DOC', 'E_COLI_MPN', 'E_COLI_MPN_FLAG', 'F', 'F_FLAG', 'FE', 'FE_FLAG', 'HARD', 'HG', 'HG_FLAG', 'K', 'LI', 'MG', 'MN', 'MN_FLAG', 'MO', 'MO_FLAG', 'NH3T', 'NH3T_FLAG', 'NH3_UN_ION', 'NH3_UN_ION_FLAG', 'NO2', 'NO2_FLAG', 'NO3', 'NO3_FLAG', 'NA', 'NI', 'NI_FLAG', 'PB', 'PB_FLAG', 'RB', 'SO4', 'SO4_FLAG', 'SS', 'SS_FLAG', 'SB', 'SB_FLAG', 'SE', 'SE_FLAG', 'SECCHIDEP', 'SN', 'SN_FLAG', 'SR', 'TC_MPN', 'TEMP', 'TKN', 'TKN_FLAG', 'TN', 'TN_FLAG', 'TOC', 'TOC_FLAG', 'TP_L', 'TP_L_FLAG', 'TURB', 'TURB_FLAG', 'TE', 'TE_FLAG', 'TL', 'TL_FLAG', 'U', 'U_FLAG', 'V', 'V_FLAG', 'ZN', 'ZN_FLAG', 'PH', 'TDS_CALC']\n",
    "\n",
    "#lakes_program_p3[cols] = lakes_program_p3[cols].replace(np.nan,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty String to NaN\n",
    "\n",
    "#cols = ['STATION_ID','RESULT_ID_DEPTH', 'DATE_TIME', 'YEAR', 'STATION_NAME', 'LATITUDE', 'LONGITUDE', 'ALK_G', 'ALK_T', 'AG', 'AG_FLAG', 'AL', 'AS', 'AS_FLAG', 'B', 'B_FLAG', 'BE_X', 'BE_X_FLAG', 'BR2', 'BR2_FLAG', 'BA', 'BA_FLAG', 'BI', 'BI_FLAG', 'BR', 'BR_FLAG', 'CHL_A', 'CHL_A_FLAG', 'CLRA', 'CLRA_FLAG', 'CLRT', 'CLRT_FLAG', 'COND', 'CA', 'CD', 'CD_FLAG', 'CL', 'CO', 'CO_FLAG', 'CR', 'CR_FLAG', 'CU', 'CU_FLAG', 'DO', 'DOC', 'E_COLI_MPN', 'E_COLI_MPN_FLAG', 'F', 'F_FLAG', 'FE', 'FE_FLAG', 'HARD', 'HG', 'HG_FLAG', 'K', 'LI', 'MG', 'MN', 'MN_FLAG', 'MO', 'MO_FLAG', 'NH3T', 'NH3T_FLAG', 'NH3_UN_ION', 'NH3_UN_ION_FLAG', 'NO2', 'NO2_FLAG', 'NO3', 'NO3_FLAG', 'NA', 'NI', 'NI_FLAG', 'PB', 'PB_FLAG', 'RB', 'SO4', 'SO4_FLAG', 'SS', 'SS_FLAG', 'SB', 'SB_FLAG', 'SE', 'SE_FLAG', 'SECCHIDEP', 'SN', 'SN_FLAG', 'SR', 'TC_MPN', 'TEMP', 'TKN', 'TKN_FLAG', 'TN', 'TN_FLAG', 'TOC', 'TOC_FLAG', 'TP_L', 'TP_L_FLAG', 'TURB', 'TURB_FLAG', 'TE', 'TE_FLAG', 'TL', 'TL_FLAG', 'U', 'U_FLAG', 'V', 'V_FLAG', 'ZN', 'ZN_FLAG', 'PH', 'TDS_CALC']\n",
    "\n",
    "#lakes_program_p3[cols] = lakes_program_p3[cols].replace(\"\",np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lakes_program_p3['E_COLI_MPN_FLAG'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null columns\n",
    "lakes_program = drop_empty_columns_dataset(lakes_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate columns \n",
    "#cols = ['STATION_ID','RESULT_ID_DEPTH', 'DATE_TIME', 'YEAR', 'STATION_NAME', 'LATITUDE', 'LONGITUDE', 'ALK_G', 'ALK_T', 'AG', 'AG_FLAG', 'AL', 'AS', 'AS_FLAG', 'B', 'B_FLAG', 'BE_X', 'BE_X_FLAG', 'BR2', 'BR2_FLAG', 'BA', 'BA_FLAG', 'BI', 'BI_FLAG', 'BR', 'BR_FLAG', 'CHL_A', 'CHL_A_FLAG', 'CLRA', 'CLRA_FLAG', 'CLRT', 'CLRT_FLAG', 'COND', 'CA', 'CD', 'CD_FLAG', 'CL', 'CO', 'CO_FLAG', 'CR', 'CR_FLAG', 'CU', 'CU_FLAG', 'DO', 'DOC', 'E_COLI_MPN', 'E_COLI_MPN_FLAG', 'F', 'F_FLAG', 'FE', 'FE_FLAG', 'HARD', 'HG', 'HG_FLAG', 'K', 'LI', 'MG', 'MN', 'MN_FLAG', 'MO', 'MO_FLAG', 'NH3T', 'NH3T_FLAG', 'NH3_UN_ION', 'NH3_UN_ION_FLAG', 'NO2', 'NO2_FLAG', 'NO3', 'NO3_FLAG', 'NA', 'NI', 'NI_FLAG', 'PB', 'PB_FLAG', 'RB', 'SO4', 'SO4_FLAG', 'SS', 'SS_FLAG', 'SB', 'SB_FLAG', 'SE', 'SE_FLAG', 'SECCHIDEP', 'SN', 'SN_FLAG', 'SR', 'TC_MPN', 'TEMP', 'TKN', 'TKN_FLAG', 'TN', 'TN_FLAG', 'TOC', 'TOC_FLAG', 'TP_L', 'TP_L_FLAG', 'TURB', 'TURB_FLAG', 'TE', 'TE_FLAG', 'TL', 'TL_FLAG', 'U', 'U_FLAG', 'V', 'V_FLAG', 'ZN', 'ZN_FLAG', 'PH', 'TDS_CALC']\n",
    "\n",
    "#recreational_beach_monitoring_p4 = recreational_beach_monitoring_p4.drop(cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange columns\n",
    "\n",
    "cols = ['STATION_ID', 'STATION_NAME', 'DATE_TIME', 'DEPTH_M', 'LATITUDE', 'LONGITUDE', 'YEAR', 'ALK_G', 'ALK_G_SOURCE', 'ALK_T', 'ALK_T_SOURCE', 'AG', 'AG_FLAG', 'AG_SOURCE', 'AL', 'AL_SOURCE', 'AS', 'AS_FLAG', 'AS_SOURCE', 'B', 'B_FLAG', 'B_SOURCE', 'BE_X', 'BE_X_FLAG', 'BE_X_SOURCE', 'BR2', 'BR2_FLAG', 'BR2_SOURCE', 'BA', 'BA_FLAG', 'BA_SOURCE', 'BI', 'BI_FLAG', 'BI_SOURCE', 'BR', 'BR_FLAG', 'BR_SOURCE', 'CHL_A', 'CHL_A_FLAG', 'CHL_A_SOURCE', 'CLRA', 'CLRA_FLAG', 'CLRA_SOURCE', 'CLRT', 'CLRT_FLAG', 'CLRT_SOURCE', 'COND', 'COND_SOURCE', 'CA', 'CA_SOURCE', 'CD', 'CD_FLAG', 'CD_SOURCE', 'CL', 'CL_SOURCE', 'CO', 'CO_FLAG', 'CO_SOURCE', 'CR', 'CR_FLAG', 'CR_SOURCE', 'CU', 'CU_FLAG', 'CU_SOURCE', 'DO', 'DO_SOURCE', 'DOC', 'DOC_SOURCE', 'E_COLI_MPN', 'E_COLI_MPN_FLAG', 'E_COLI_MPN_SOURCE', 'F', 'F_FLAG', 'F_SOURCE', 'FE', 'FE_FLAG', 'FE_SOURCE', 'HARD', 'HARD_SOURCE', 'HG', 'HG_FLAG', 'HG_SOURCE', 'K', 'K_SOURCE', 'LI', 'LI_SOURCE', 'MG', 'MG_SOURCE', 'MN', 'MN_FLAG', 'MN_SOURCE', 'MO', 'MO_FLAG', 'MO_SOURCE', 'NH3T', 'NH3T_FLAG', 'NH3T_SOURCE', 'NH3_UN_ION', 'NH3_UN_ION_FLAG', 'NH3_UN_ION_SOURCE', 'NO2', 'NO2_FLAG', 'NO2_SOURCE', 'NO3', 'NO3_FLAG', 'NO3_SOURCE', 'NA', 'NA_SOURCE', 'NI', 'NI_FLAG', 'NI_SOURCE', 'PB', 'PB_FLAG', 'PB_SOURCE', 'RB', 'RB_SOURCE', 'SO4', 'SO4_FLAG', 'SO4_SOURCE', 'SS', 'SS_FLAG', 'SS_SOURCE', 'SB', 'SB_FLAG', 'SB_SOURCE', 'SE', 'SE_FLAG', 'SE_SOURCE', 'SECCHIDEP', 'SECCHIDEP_SOURCE', 'SN', 'SN_FLAG', 'SN_SOURCE', 'SR', 'SR_SOURCE', 'TC_MPN', 'TC_MPN_SOURCE', 'TEMP', 'TEMP_SOURCE', 'TKN', 'TKN_FLAG', 'TKN_SOURCE', 'TN', 'TN_FLAG', 'TN_SOURCE', 'TOC', 'TOC_FLAG', 'TOC_SOURCE', 'TP_L', 'TP_L_FLAG', 'TP_L_SOURCE', 'TURB', 'TURB_FLAG', 'TURB_SOURCE', 'TE', 'TE_FLAG', 'TE_SOURCE', 'TL', 'TL_FLAG', 'TL_SOURCE', 'U', 'U_FLAG', 'U_SOURCE', 'V', 'V_FLAG', 'V_SOURCE', 'ZN', 'ZN_FLAG', 'ZN_SOURCE', 'PH', 'PH_SOURCE', 'TDS_CALC', 'TDS_CALC_SOURCE']\n",
    "\n",
    "lakes_program = lakes_program[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Combined Dataset to a CSV\n",
    "\n",
    "lakes_program.to_csv(\"data/lakes-program.csv\", sep=',',index=False,encoding='utf-8-sig')\n",
    "\n",
    "#Shape of row data\n",
    "lakes_program.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program.columns.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot null values\n",
    "def plot_null_values(dataset,group_by,year_filter_switch, year_filter):\n",
    "    if year_filter_switch:\n",
    "        dataset = dataset[(dataset['YEAR'] == year_filter)]\n",
    "    \n",
    "    dataset = dataset.groupby([group_by])\n",
    "    # extract keys from groups\n",
    "    keys = dataset.groups.keys()\n",
    "\n",
    "    totalCols=1\n",
    "    totalRows=math.ceil(len(dataset)/totalCols)\n",
    "    \n",
    "    fig = plt.figure(figsize=((totalCols+3)*4,(totalRows+1)*5))\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.7)\n",
    "\n",
    "    for index, x in enumerate(keys):\n",
    "        null_columns = []\n",
    "        null_column_values = []\n",
    "\n",
    "        #print(dataset.get_group(x).columns[2])\n",
    "\n",
    "        for i in range(0,len(dataset.get_group(x).columns),1):\n",
    "            if(dataset[dataset.get_group(x).columns[i]].get_group(x).isna().sum() > 0):\n",
    "                null_columns.append(dataset.get_group(x).columns[i])\n",
    "                null_column_values.append(dataset[dataset.get_group(x).columns[i]].get_group(x).isna().sum())\n",
    "\n",
    "        globals()[f\"ax_count_plots_{index}\"] = fig.add_subplot(totalRows,totalCols,(index+1))\n",
    "        globals()[f\"ax_count_plots_{index}\"].set_title(x.upper(),backgroundcolor='gray')\n",
    "        \n",
    "\n",
    "        globals()[f\"ax_count_plots_{index}\"].bar(null_columns,null_column_values)\n",
    "        \n",
    "        globals()[f\"ax_count_plots_{index}\"].set(xlabel=None)\n",
    "        globals()[f\"ax_count_plots_{index}\"].tick_params(axis='x', labelrotation = 90)\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_null_values(lakes_program,'STATION_NAME',True,2020) #false if dont want to use year filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "pd.set_option('display.max_rows',None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "lakes_program.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This is just an example of a line graph, visualization can be better\n",
    "def linechart_of_categories(dataset,group_by,time_column,value_column,year_filter_switch,year_filter):\n",
    "    if year_filter_switch:\n",
    "        dataset = dataset[(dataset['YEAR'] == year_filter)]\n",
    "\n",
    "    dataset = dataset.set_index(time_column)\n",
    "    dataset = dataset.groupby([group_by])\n",
    "    # extract keys from groups\n",
    "    keys = dataset.groups.keys()\n",
    "    totalRows = 0\n",
    "    for index, x in enumerate(keys):\n",
    "        if(len(dataset[value_column].get_group(x))!=dataset[value_column].get_group(x).isna().sum()):\n",
    "            totalRows+=1\n",
    "    \n",
    "    totalCols=3\n",
    "    totalRows=math.ceil(totalRows/totalCols)\n",
    "    \n",
    "    fig = plt.figure(figsize=((totalCols+3)*3,(totalRows+1)*5))\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.6)\n",
    "    newInx = 1\n",
    "    for index, x in enumerate(keys):\n",
    "        if(len(dataset[value_column].get_group(x))!=dataset[value_column].get_group(x).isna().sum()):\n",
    "            globals()[f\"ax_count_plots_{index}\"] = fig.add_subplot(totalRows,totalCols,newInx)\n",
    "            globals()[f\"ax_count_plots_{index}\"].set_title(x.upper())\n",
    "            #if(len(dataset[value_column].get_group(x))!=dataset[value_column].get_group(x).isna().sum()):\n",
    "            dataset[value_column].get_group(x).plot()\n",
    "            \n",
    "            globals()[f\"ax_count_plots_{index}\"].set(xlabel=None)\n",
    "            globals()[f\"ax_count_plots_{index}\"].tick_params(axis='x', labelrotation = 90)\n",
    "            newInx+=1\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linechart_of_categories(lakes_program,'STATION_NAME','DATE_TIME','ALK_G',False,2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display unique values\n",
    "\n",
    "def unique_values__or_count(listOfColumns,options,dataset):\n",
    "    for x in range(0, len(listOfColumns), 1):\n",
    "        if(options==\"unique\"):\n",
    "            unique_values_str = dataset[listOfColumns[x]].unique()\n",
    "            print(\"unique_values \" + listOfColumns[x])\n",
    "            print(unique_values_str)\n",
    "            print(\"------------------------\")\n",
    "        if(options==\"count\"):\n",
    "            values_distribution = dataset[listOfColumns[x]].value_counts()\n",
    "            print(\"-----------\"+listOfColumns[x] +\"------------\")\n",
    "            print(values_distribution)\n",
    "            print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check station values\n",
    "\n",
    "unique_values__or_count(['STATION_NAME'],\"unique\",lakes_program)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import cleaned data \n",
    "\n",
    "lakes_program_validate = pd.read_csv(\"data/lakes-program.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_validate.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Col_name_to_validate = \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_validate[\"DATE_TIME\"] = pd.to_datetime(lakes_program_validate[\"DATE_TIME\"],format='%Y-%m-%d %H:%M:%S')\n",
    "lakes_program_validate = lakes_program_validate[[\"STATION_ID\", \"DATE_TIME\",\"DEPTH_M\", Col_name_to_validate]].copy()\n",
    "lakes_program_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_validate = lakes_program_validate.dropna(subset = [Col_name_to_validate])\n",
    "lakes_program_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import original data \n",
    "\n",
    "lakes_program_original = pd.read_csv(\"raw_data/data.csv\", low_memory=False, sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"List of Columns\")\n",
    "print(lakes_program_original.columns.to_list())\n",
    "print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_validate_with = \"F\"\n",
    "lakes_program_original = lakes_program_original[(lakes_program_original['PARMABBR'] == col_to_validate_with)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_original = lakes_program_original[[\"STATIONID\", \"RESULTDATE\",\"RESULTID\", \"PARMABBR\",\"RESULTVAL\",\"RESULTDEPTH\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns \n",
    "lakes_program_original.rename(columns={'STATIONID': 'STATION_ID', 'RESULTDATE': 'DATE_TIME',\"RESULTID\":\"RESULT_ID\",\"RESULTDEPTH\":\"DEPTH_M\",\"RESULTVAL\":Col_name_to_validate}, inplace=True)\n",
    "\n",
    "#rakes_program_original = rakes_program_original.rename(columns=lambda x: clean_column_names(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change date format\n",
    "lakes_program_original[\"DATE_TIME\"] = pd.to_datetime(lakes_program_original[\"DATE_TIME\"],format='%Y/%m/%d %I.%M.%S.%f %p')\n",
    "\n",
    "#Trim data to validate an analyte\n",
    "lakes_program_original = lakes_program_original[[\"STATION_ID\", \"DATE_TIME\", \"DEPTH_M\",Col_name_to_validate]].copy()\n",
    "lakes_program_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lakes_program_validate.info()\n",
    "lakes_program_original = lakes_program_original.dropna(subset = [Col_name_to_validate])\n",
    "lakes_program_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_program_validate_results = pd.merge(lakes_program_validate, lakes_program_original, on=[\"STATION_ID\", \"DATE_TIME\",\"DEPTH_M\",Col_name_to_validate], how='right', indicator='Exist')\n",
    "lakes_program_validate_results['Exist'] = np.where(lakes_program_validate_results.Exist == 'both', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values__or_count(['Exist'],\"count\",lakes_program_validate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_missing_rows = lakes_program_validate_results[(lakes_program_validate_results['Exist'] == False)].copy()\n",
    "\n",
    "list_of_missing_rows.head(5)\n",
    "\n",
    "#lakes_program_validate_results.to_csv(\"data/temp.csv\", sep=',',index=False,encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c481cd3fc83eb5588546703e9d94f1058347710cd104ca041365c752440aa931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
